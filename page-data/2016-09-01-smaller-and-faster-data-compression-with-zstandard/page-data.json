{"componentChunkName":"component---src-templates-blog-post-js","path":"/2016-09-01-smaller-and-faster-data-compression-with-zstandard","webpackCompilationHash":"ea9e41e646c8f86c1300","result":{"data":{"site":{"siteMetadata":{"title":"kevinhakanson.com","author":"Kevin Hakanson"}},"markdownRemark":{"id":"ef2fef3e-477a-5c15-b1a6-ab374e058545","excerpt":"I saw this post from Facebook on their new open source compression algorithm: Smaller and faster data compression with Zstandard | Engineering Blog | Facebook…","html":"<p>I saw this post from Facebook on their new open source compression algorithm: <a href=\"https://code.facebook.com/posts/1658392934479273\">Smaller and faster data compression with Zstandard | Engineering Blog | Facebook Code</a>.</p>\n<blockquote>\n<p>We’re thrilled to announce <a href=\"https://github.com/facebook/zstd\">Zstandard 1.0</a>, a new compression algorithm and implementation designed to scale with modern hardware and compress smaller and faster. Zstandard combines recent compression breakthroughs, like <a href=\"https://github.com/Cyan4973/FiniteStateEntropy\">Finite State Entropy</a>, with a performance-first design — and then optimizes the implementation for the unique properties of modern CPUs. As a result, it improves upon the trade-offs made by other compression algorithms and has a wide range of applicability with very high decompression speed. Zstandard, available now under the BSD license, is designed to be used in nearly every lossless compression [1] scenario, including many where current algorithms aren’t applicable.</p>\n</blockquote>\n<p>The “internet standard” today is Deflate, which is used by zip, gzip, and zlib. When compared to zlib, Facebook shows that zstandard scales much better:</p>\n<blockquote>\n<ul>\n<li>At the same compression ratio, it compresses substantially faster: ~3-5x.</li>\n<li>At the same compression speed, it is substantially smaller: 10-15 percent smaller.</li>\n<li>It is almost 2x faster at decompression, regardless of compression ratio; the command line tooling numbers show an even bigger difference: more than 3x faster.</li>\n<li>It scales to much higher compression ratios, while sustaining lightning-fast decompression speeds.</li>\n</ul>\n</blockquote>\n<p>The article continues with an “under the hood” section talking about “parallel execution” and “branchless design” and how those are important to today’s CPU architectures.  Good reading for those propeller-heads out there.</p>\n<p>One of their highlighted use cases is storing “small data”; some examples are pages in a database or JSON documents.  They also mention that compressed data stored on SSDs help with both IOPs and the write endurance of the hardware.  Facebook stores a lot of data and buys a lot of SSDs, so I can see how this will save them money.</p>","frontmatter":{"title":"Smaller and faster data compression with Zstandard","date":"September 01, 2016","tags":["webdev"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/2016-09-01-smaller-and-faster-data-compression-with-zstandard","previous":{"fields":{"slug":"/2016-08-22-investigating-cloudtrail-for-s3-putbucketnotification"},"frontmatter":{"tags":["aws","cloudtrail","s3"]}},"next":{"fields":{"slug":"/2016-09-06-hpkp-and-ransompkp"},"frontmatter":{"tags":["http","security","webdev"]}}}}}