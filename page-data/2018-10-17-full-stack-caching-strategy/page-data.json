{"componentChunkName":"component---src-templates-blog-post-js","path":"/2018-10-17-full-stack-caching-strategy","webpackCompilationHash":"af0e4c24d4a507da8bf9","result":{"data":{"site":{"siteMetadata":{"title":"kevinhakanson.com","author":"Kevin Hakanson"}},"markdownRemark":{"id":"fc577e22-8d61-5dbd-8dff-56562f4da413","excerpt":"“Caching is somebody else’s problem, until it’s not” - me (just now) OK, so that’s a dumb quote I just made up, until it’s not.  Microservices do their work…","html":"<blockquote>\n<p><em>“Caching is somebody else’s problem, until it’s not” - me (just now)</em></p>\n</blockquote>\n<p>OK, so that’s a dumb quote I just made up, until it’s not.  Microservices do their work, generate their HTTP, response and sent it to the browser.  Often, it isn’t until there is a performance or scale problem before a caching strategy gets addressed.</p>\n<p>How might a “full stack” caching strategy work?  Take a look at the representative diagram below:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/060abe886d7de5d435e5ba957a004142/f4f98/pastedImage_5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.578947368421055%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABibAAAYmwFJdYOUAAABe0lEQVQY012Ry27TUBCG/ZLwAOxY8wwg6IKbwhK1G3qRSiliA0VIqYCAq9xKCSUhjnNzmrq1G9+Oj+PLx4kroYqRRrP4R59m/l9bJksSkbKUKSKKsM4uCMKYbJmTJQUr/WYVRUGhZrbqLCPt18gCh7wUc7QgDNB7B+hmlfFsxJ/2a+ajBo2uzsf6LrZjk0mB8fYFkVEvofGgjvfrM1JRrtbv4lZu413OS02LREh7+J1aq4LZesig8ZxR/REt/T7N/hdm9gzPGnD8+A7jgw18BXH3H+Bu3isB/uQU6+ktUqtzDQxDn3e1bfY+rTEb7GEq4OTnSwi75UIQ5eU7ncP3OJMu02GDsfkD87BKd3cH4QsuvIjKGx3bk2giFnSMJidGG2fhYVsnuJdThLJOBiPOe1uc/X6FCA3iRHJ6tMHUqBLNDazmB4TTJ5aSZ/tN5osY7T/H/41cnZVFJm6vwnnnCYnzlTSFRIWUqkTk1TcWw3X88TapXJCrgFah/AU0T7UJuGK4wgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"pastedImage 5\"\n        title=\"pastedImage 5\"\n        src=\"/static/060abe886d7de5d435e5ba957a004142/b9e4f/pastedImage_5.png\"\n        srcset=\"/static/060abe886d7de5d435e5ba957a004142/cf440/pastedImage_5.png 148w,\n/static/060abe886d7de5d435e5ba957a004142/d2d38/pastedImage_5.png 295w,\n/static/060abe886d7de5d435e5ba957a004142/b9e4f/pastedImage_5.png 590w,\n/static/060abe886d7de5d435e5ba957a004142/f9b6a/pastedImage_5.png 885w,\n/static/060abe886d7de5d435e5ba957a004142/f4f98/pastedImage_5.png 1140w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Let’s start with the microservice and follow the response out.  How much of a response could benefit from a cache?  If it is partially built from of a query result, look at an external cache that works with your database.  If it is the entire HTTP response, look at something that works with your web server pipeline.  However, with all shared caches, think about security and privacy so you don’t disclose one customer’s data to another.  Some additional links:</p>\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/redis-cache/cache-aspnet-output-cache-provider\">Cache ASP.NET Output Cache Provider | Microsoft Docs</a></li>\n<li><a href=\"http://nhibernate.info/doc/nhibernate-reference/caches.html#NHibernate.Caches.CoreDistributedCache.Redis\">Chapter 26. NHibernate.Caches</a></li>\n</ul>\n<p>The response flows through NGINX, which has <a href=\"https://www.nginx.com/products/nginx/caching/\">Content caching</a> functionality.  However, this might not be the best place in the architecture for an application with lots of microservices.  Some additional links:</p>\n<ul>\n<li><a href=\"https://docs.nginx.com/nginx/admin-guide/content-cache/content-caching/\">NGINX Docs | NGINX Content Caching</a></li>\n<li><a href=\"https://www.nginx.com/blog/nginx-caching-guide/\">A Guide to Caching with NGINX and NGINX Plus - NGINX</a>  </li>\n</ul>\n<p>The response may flow through a content delivery network (like CloudFront) where it could get cached on the “edge” based on the HTTP cache headers.  Early CDNs required you to push out “static content” and use a discrete hostname for browser connection limit reasons.  However, mature CDNs allow for “dynamic acceleration” where requests flow through and utilize HTTP cache headers.  HTTP/2 connections make a single hostname the preferred option.  These distributed shared caches are closer to the browser and give a network latency benefit.  Some additional links:</p>\n<ul>\n<li><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html\">Caching Content Based on Request Headers - Amazon CloudFront</a></li>\n<li><a href=\"https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_Headers.html\">Headers - Amazon CloudFront</a> </li>\n<li><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\">Managing How Long Content Stays in an Edge Cache (Expiration) - Amazon CloudFront</a></li>\n</ul>\n<p>Once the response gets to the browser, there is another ecosystem in play.  The browser has a private cache (populated by HTTP cache headers) and localStorage/indexedDB (populated by JavaScript code).  Additionally, “evergreen” browsers now support Service Workers which can intercept HTTP requests and have access to a Cache API.  Some additional links:</p>\n<ul>\n<li><a href=\"https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching\">HTTP Caching  |  Web Fundamentals  |  Google Developers</a></li>\n<li><a href=\"https://developers.google.com/web/ilt/pwa/caching-files-with-service-worker\">Caching Files with Service Worker  |  Web  |  Google Developers</a></li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers\">Using Service Workers - Web APIs | MDN</a></li>\n</ul>\n<p>Any place that has a shared cache needs to be concerned about the cache key.  This is more than just the URL, since different users might get different results (e.g. /profile/me) or different Content Types might be returned based on Accept headers.  The Vary header is often used as part of the cache key.  Some additional links:</p>\n<ul>\n<li><a href=\"https://www.fastly.com/blog/getting-most-out-vary-fastly\">Getting the most out of Vary with Fastly</a> </li>\n<li><a href=\"https://www.smashingmagazine.com/2017/11/understanding-vary-header/\">Understanding The Vary Header — Smashing Magazine</a> </li>\n</ul>\n<p>This was just a quick overview as there is a lot more complexity to this topic.  What key parts are missing?  Where in the architecture makes the most sense to you, and how should a development team decide where caching belongs?</p>","frontmatter":{"title":"\"Full Stack\" Caching Strategy","date":"October 17, 2018","tags":["webdev","caching","http"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/2018-10-17-full-stack-caching-strategy","previous":{"fields":{"slug":"/2018-10-11-x-xss-protection-is-dead-long-live-content-security-policy"},"frontmatter":{"tags":["http","webdev","security"]}},"next":{"fields":{"slug":"/2018-11-08-excited-about-azure-signalr-serviceaspnet-core-signalr"},"frontmatter":{"tags":["azure","dotnet","podcast"]}}}}}